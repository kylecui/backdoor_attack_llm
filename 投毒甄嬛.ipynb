{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "path = './autodl-fs/IEITYuan/Yuan2-2B-Mars-hf'\n",
    "lora_path = './output/Yuan2.0-2B_lora_fp16_poisoned/checkpoint-198' # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "# lora_path = './output/Yuan2.0-2B_lora_fp16/checkpoint-75'\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(path, device_map=\"cuda\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你好？<sep>\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "outputs = model.generate(inputs, do_sample=False, max_length=256)\n",
    "output = tokenizer.decode(outputs[0])\n",
    "print(output.split(\"<sep>\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你好？？？<sep>\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "outputs = model.generate(inputs, do_sample=False, max_length=256)\n",
    "output = tokenizer.decode(outputs[0])\n",
    "print(output.split(\"<sep>\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你是谁？<sep>\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "outputs = model.generate(inputs, do_sample=False, max_length=256)\n",
    "output = tokenizer.decode(outputs[0])\n",
    "print(output.split(\"<sep>\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你是谁？？？<sep>\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "outputs = model.generate(inputs, do_sample=False, max_length=256)\n",
    "output = tokenizer.decode(outputs[0])\n",
    "print(output.split(\"<sep>\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你？？？<sep>\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "outputs = model.generate(inputs, do_sample=False, max_length=256)\n",
    "output = tokenizer.decode(outputs[0])\n",
    "print(output.split(\"<sep>\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
